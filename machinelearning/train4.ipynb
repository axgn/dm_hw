{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dadf9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings = pd.read_csv(\"../datasets/ml-25m/ratings.csv\")\n",
    "movies = pd.read_csv(\n",
    "    \"../datasets/ml-25m/movies.csv\",\n",
    ")\n",
    "df = pd.merge(ratings, movies[[\"movieId\", \"title\"]], on=\"movieId\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c4203b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162541, 59047)\n",
      "25000095\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# 编码（避免巨大稀疏索引）\n",
    "user_codes = df[\"userId\"].astype(\"category\").cat.codes\n",
    "movie_codes = df[\"movieId\"].astype(\"category\").cat.codes\n",
    "\n",
    "ratings_sparse = csr_matrix((df[\"rating\"], (user_codes, movie_codes)))\n",
    "\n",
    "print(ratings_sparse.shape)\n",
    "print(ratings_sparse.nnz)  # 非零元素数量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ed9c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160915, 59047) (1626, 59047)\n",
      "24749602 250493\n"
     ]
    }
   ],
   "source": [
    "def splite_data(ratings_sparse, test_size=0.2):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    train_data, test_data = train_test_split(\n",
    "        range(ratings_sparse.shape[0]),\n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "    )\n",
    "    return train_data, test_data\n",
    "\n",
    "train_idx, test_idx = splite_data(ratings_sparse,test_size=0.01)\n",
    "train_data = ratings_sparse[train_idx]\n",
    "test_data = ratings_sparse[test_idx]\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_data.nnz, test_data.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ac86c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1626, 59047) (1626, 59047)\n",
      "201012 250493\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "val_data = test_data.copy()\n",
    "def mask_test_data(test_data, frac=0.2):\n",
    "    data = test_data.data\n",
    "    indices = test_data.indices\n",
    "    indptr = test_data.indptr\n",
    "    shape = test_data.shape\n",
    "    \n",
    "    mask = np.ones_like(data, dtype=bool)\n",
    "    for i in range(test_data.shape[0]):\n",
    "        start = indptr[i]\n",
    "        end = indptr[i + 1]\n",
    "        n_ratings = end - start\n",
    "        n_mask = max(1, int(n_ratings * frac))\n",
    "        if n_ratings > 0:\n",
    "            mask_indices = np.random.choice(\n",
    "                np.arange(start, end), size=n_mask, replace=False\n",
    "            )\n",
    "            mask[mask_indices] = False\n",
    "    data_masked = data[mask]\n",
    "    indices_masked = indices[mask]\n",
    "    indptr_masked = np.zeros(shape[0] + 1, dtype=indptr.dtype)\n",
    "    retained_positions = np.where(mask)[0]\n",
    "    row_assignments = np.searchsorted(indptr, retained_positions, side=\"right\") - 1\n",
    "    row_counts = np.bincount(row_assignments, minlength=shape[0])\n",
    "    cumulative_sum = np.cumsum(row_counts)\n",
    "    indptr_masked[1:] = cumulative_sum\n",
    "    return csr_matrix((data_masked, indices_masked, indptr_masked), shape=test_data.shape)\n",
    "\n",
    "test_data = mask_test_data(test_data, frac=0.2)\n",
    "print(test_data.shape, val_data.shape)\n",
    "print(test_data.nnz, val_data.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c2e2a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(74702, 0.8115955591201782), (82317, 0.8071398735046387), (57888, 0.7959361672401428), (29629, 0.7842761278152466), (99576, 0.7763625979423523), (57636, 0.7716629505157471), (112559, 0.7714238166809082), (9605, 0.7642025351524353), (101887, 0.7566736340522766), (64357, 0.7540217638015747), (81077, 0.7518483400344849), (91539, 0.7490201592445374), (31418, 0.743718683719635), (21975, 0.7339298725128174), (20690, 0.7337057590484619), (95734, 0.7261963486671448), (82974, 0.7246642112731934), (30153, 0.7174654603004456), (135650, 0.715464174747467), (22494, 0.7121829986572266), (114861, 0.7100189924240112), (86831, 0.7018918395042419), (57081, 0.6998987197875977), (93155, 0.6988957524299622), (40605, 0.6986770033836365), (5784, 0.6978859901428223), (36425, 0.6964825391769409), (94396, 0.6920182108879089), (98849, 0.6873684525489807), (141180, 0.6872909069061279), (147467, 0.6863797903060913), (86858, 0.6847625374794006), (141787, 0.6842553615570068), (4601, 0.6837467551231384), (35430, 0.6814340353012085), (77173, 0.6801561117172241), (82746, 0.6794837713241577), (62232, 0.6781785488128662), (105759, 0.6771878004074097), (86477, 0.6753584146499634), (7671, 0.6745730638504028), (85647, 0.6727162599563599), (49429, 0.6726469397544861), (788, 0.6720718145370483), (78242, 0.6706684827804565), (44163, 0.6676544547080994), (122720, 0.6673797369003296), (113321, 0.6672607064247131), (132195, 0.6663157939910889), (71063, 0.6654942631721497)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "import hnswlib\n",
    "\n",
    "\n",
    "def hnswlib_topk_svd(train_sparse,test_sparse, k=50, dim=128):\n",
    "    svd = TruncatedSVD(n_components=dim)\n",
    "    train = svd.fit_transform(train_sparse).astype(\"float32\")\n",
    "\n",
    "    train_norm = normalize(train, axis=1).astype(\"float32\")\n",
    "    n_users_train, dim = train_norm.shape\n",
    "    n_users_test = test_sparse.shape[0]\n",
    "    test = svd.transform(test_sparse).astype(\"float32\")\n",
    "    test_norm = normalize(test, axis=1).astype(\"float32\")\n",
    "\n",
    "    index = hnswlib.Index(space=\"cosine\", dim=dim)\n",
    "    index.init_index(max_elements=n_users_train, ef_construction=200, M=32)\n",
    "    index.add_items(train_norm)\n",
    "    index.set_ef(128)\n",
    "\n",
    "    top_ids, top_dists = index.knn_query(test_norm, k + 1)\n",
    "\n",
    "    topk = {}\n",
    "    for u in range(n_users_test):\n",
    "        ids = top_ids[u]\n",
    "        dists = top_dists[u]\n",
    "        topk[u] = list(zip(ids[:k].tolist(), (1 - dists[:k]).tolist()))\n",
    "    return topk\n",
    "\n",
    "\n",
    "topk_dict = hnswlib_topk_svd(train_data, test_data, k=50)\n",
    "print(topk_dict[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be36dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1746\n"
     ]
    }
   ],
   "source": [
    "def rmse(topk_dict, val_data):\n",
    "    from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # 将train_data转换为易于访问的形式\n",
    "    train_ratings = {}\n",
    "    for u in range(train_data.shape[0]):\n",
    "        start = train_data.indptr[u]\n",
    "        end = train_data.indptr[u + 1]\n",
    "        for i in range(start, end):\n",
    "            movie = train_data.indices[i]\n",
    "            rating = train_data.data[i]\n",
    "            train_ratings[(u, movie)] = rating\n",
    "\n",
    "    # 对验证集中的每个用户进行预测\n",
    "    for u in range(val_data.shape[0]):\n",
    "        start = val_data.indptr[u]\n",
    "        end = val_data.indptr[u + 1]\n",
    "        movie_indices = val_data.indices[start:end]\n",
    "        true_ratings = val_data.data[start:end]\n",
    "\n",
    "        if len(true_ratings) == 0:\n",
    "            continue\n",
    "\n",
    "        # 获取用户的最近邻及相似度\n",
    "        neighbors = topk_dict.get(u, [])\n",
    "\n",
    "        # 对用户观看的每部电影进行预测\n",
    "        for movie, true_rating in zip(movie_indices, true_ratings):\n",
    "            numerator = 0.0  # 分子：加权评分总和\n",
    "            denominator = 0.0  # 分母：相似度总和\n",
    "\n",
    "            # 基于邻居的评分进行加权平均\n",
    "            for neighbor_id, similarity in neighbors:\n",
    "                neighbor_rating = train_ratings.get((neighbor_id, movie), None)\n",
    "                if neighbor_rating is not None:\n",
    "                    numerator += similarity * neighbor_rating\n",
    "                    denominator += abs(similarity)\n",
    "\n",
    "            # 预测评分\n",
    "            if denominator > 0:\n",
    "                predicted_rating = numerator / denominator\n",
    "                # 确保评分在合理范围内（如1-5）\n",
    "                predicted_rating = max(1.0, min(5.0, predicted_rating))\n",
    "            else:\n",
    "                predicted_rating = 0  # 或使用全局平均分\n",
    "\n",
    "            y_true.append(true_rating)\n",
    "            y_pred.append(predicted_rating)\n",
    "\n",
    "    rmse_value = root_mean_squared_error(y_true, y_pred)\n",
    "    return rmse_value\n",
    "\n",
    "rmse_value = rmse(topk_dict, val_data)\n",
    "print(f\"RMSE: {rmse_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43761340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于邻居的召回率: 0.3425\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def recall_from_user_neighbors(topk_dict, train_mat, test_mat, N=50, like_threshold=1.0):\n",
    "    n_users = test_mat.shape[0]\n",
    "    recalls = []\n",
    "\n",
    "    for u in range(n_users):\n",
    "        # ground truth: 用户在测试集中真正喜欢的电影\n",
    "        test_row = test_mat.getrow(u)\n",
    "        liked_items = test_row.indices[test_row.data >= like_threshold]\n",
    "        if len(liked_items) == 0:\n",
    "            continue\n",
    "        liked_set = set(liked_items)\n",
    "\n",
    "        # 用户邻居\n",
    "        neighbors = topk_dict[u]\n",
    "        neighbor_ids = [nid for nid, sim in neighbors]\n",
    "\n",
    "        # 收集邻居的电影\n",
    "        movie_counter = Counter()\n",
    "        for nid in neighbor_ids:\n",
    "            row = train_mat.getrow(nid)\n",
    "            movies = row.indices  # 简单方案：不加权\n",
    "            movie_counter.update(movies)\n",
    "\n",
    "        # 去掉用户训练集里已经看过的电影（避免泄漏）\n",
    "        train_row_u = train_mat.getrow(u)\n",
    "        seen_u = set(train_row_u.indices)\n",
    "\n",
    "        # 剔除\n",
    "        for m in seen_u:\n",
    "            if m in movie_counter:\n",
    "                del movie_counter[m]\n",
    "\n",
    "        # 预测的 Top-N 电影\n",
    "        if len(movie_counter) == 0:\n",
    "            recalls.append(0.0)\n",
    "            continue\n",
    "        pred_items = [m for m, c in movie_counter.most_common(N)]\n",
    "        pred_set = set(pred_items)\n",
    "\n",
    "        # 计算召回率\n",
    "        hit = len(liked_set & pred_set)\n",
    "        recall = hit / len(liked_set)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    if len(recalls) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return np.mean(recalls)\n",
    "recall_score = recall_from_user_neighbors(topk_dict, train_data, val_data, N=50)\n",
    "print(f\"基于邻居的召回率: {recall_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm-hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
